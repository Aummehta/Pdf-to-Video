{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10975856,"sourceType":"datasetVersion","datasetId":6736117}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 1: Install Dependencies and Setup\n# ==============================================================================\nimport subprocess\nimport sys\nimport os\nimport warnings\nimport re\n\n# Suppress warnings\nos.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \"hide\"\nwarnings.filterwarnings('ignore')\n\ndef install_packages():\n    \"\"\"Install all required packages\"\"\"\n    packages = [\n        \"PyMuPDF==1.23.26\",\n        \"opencv-python-headless\", \n        \"google-generativeai\",\n        \"edge-tts\",\n        \"langchain\",\n        \"moviepy\",\n        \"Pillow\"\n    ]\n    \n    print(\"Installing required packages...\")\n    for package in packages:\n        try:\n            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n                          check=True, capture_output=True)\n            print(f\"‚úÖ {package}\")\n        except:\n            print(f\"‚ö†Ô∏è Failed to install {package}\")\n\ninstall_packages()\nprint(\"‚úÖ Cell 1 Complete - Dependencies installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:28:14.179288Z","iopub.execute_input":"2025-10-04T11:28:14.179732Z","iopub.status.idle":"2025-10-04T11:28:40.688631Z","shell.execute_reply.started":"2025-10-04T11:28:14.179700Z","shell.execute_reply":"2025-10-04T11:28:40.687581Z"}},"outputs":[{"name":"stdout","text":"Installing required packages...\n‚úÖ PyMuPDF==1.23.26\n‚úÖ opencv-python-headless\n‚úÖ google-generativeai\n‚úÖ edge-tts\n‚úÖ langchain\n‚úÖ moviepy\n‚úÖ Pillow\n‚úÖ Cell 1 Complete - Dependencies installed\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: Enhanced PDF Image Extractor (Fixed)\n# ==============================================================================\n\nimport io\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport subprocess\nimport sys\n\ndef install_and_import_pymupdf():\n    \"\"\"Install and properly import PyMuPDF\"\"\"\n    try:\n        # Clean install\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"PyMuPDF\", \"fitz\"], \n                      capture_output=True, check=False)\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"PyMuPDF==1.23.26\"], check=True)\n        \n        # Clear module cache\n        for module_name in list(sys.modules.keys()):\n            if 'fitz' in module_name.lower():\n                del sys.modules[module_name]\n        \n        import fitz\n        \n        # Test functionality\n        test_doc = fitz.open()\n        test_doc.close()\n        print(\"‚úÖ PyMuPDF installed and working\")\n        return fitz\n        \n    except Exception as e:\n        print(f\"PyMuPDF installation failed: {e}\")\n        return None\n\nclass EnhancedPDFImageExtractor:\n    def __init__(self, min_figure_size=5000):  # Reduced threshold to capture more images\n        self.min_figure_size = min_figure_size\n        self.fitz = install_and_import_pymupdf()\n        \n    def is_meaningful_image(self, pix):\n        \"\"\"Check if image is meaningful (not just background/decorative)\"\"\"\n        if pix.width < 50 or pix.height < 50:\n            return False\n        \n        # Check if image has sufficient complexity\n        area = pix.width * pix.height\n        return area >= self.min_figure_size\n        \n    def extract_all_images(self, pdf_path, output_dir=\"all_extracted_images\"):\n        \"\"\"Extract ALL images from PDF with better filtering\"\"\"\n        if not self.fitz:\n            print(\"PyMuPDF not available\")\n            return [], []\n        \n        os.makedirs(output_dir, exist_ok=True)\n        figures, metadata = [], []\n        processed_xrefs = set()  # Track processed images to avoid duplicates\n        \n        try:\n            doc = self.fitz.open(pdf_path)\n            print(f\"Processing {len(doc)} pages for image extraction...\")\n            \n            for page_num in range(len(doc)):\n                page = doc[page_num]\n                \n                # Method 1: Get all images on the page\n                image_list = page.get_images(full=True)\n                print(f\"Page {page_num + 1}: Found {len(image_list)} potential images\")\n                \n                for img_index, img in enumerate(image_list):\n                    try:\n                        xref = img[0]\n                        \n                        # Skip if already processed\n                        if xref in processed_xrefs:\n                            continue\n                        processed_xrefs.add(xref)\n                        \n                        # Get image data\n                        base_image = doc.extract_image(xref)\n                        image_bytes = base_image[\"image\"]\n                        image_ext = base_image[\"ext\"]\n                        \n                        # Create PIL image\n                        img_pil = Image.open(io.BytesIO(image_bytes))\n                        \n                        # Check if image is meaningful\n                        if img_pil.width * img_pil.height < self.min_figure_size:\n                            continue\n                            \n                        # Convert to RGB if needed\n                        if img_pil.mode != 'RGB':\n                            img_pil = img_pil.convert('RGB')\n                        \n                        # Save image\n                        save_path = os.path.join(output_dir, f\"page{page_num+1}_img{len(figures)+1}.png\")\n                        img_pil.save(save_path, \"PNG\", quality=95)\n                        \n                        figures.append(save_path)\n                        metadata.append({\n                            \"page\": page_num + 1,\n                            \"method\": \"direct_extract\",\n                            \"size\": (img_pil.width, img_pil.height),\n                            \"original_format\": image_ext\n                        })\n                        \n                        print(f\"  ‚úÖ Saved: {os.path.basename(save_path)} ({img_pil.width}x{img_pil.height})\")\n                        \n                    except Exception as e:\n                        print(f\"  ‚ö†Ô∏è Error with image {img_index + 1}: {e}\")\n                        continue\n                \n                # Method 2: Render page regions with drawings/charts\n                # This captures vector graphics that might not be in image_list\n                try:\n                    # Get page as image at high resolution\n                    mat = self.fitz.Matrix(2.0, 2.0)  # 2x zoom\n                    pix = page.get_pixmap(matrix=mat)\n                    \n                    # Convert to PIL\n                    img_data = pix.tobytes(\"png\")\n                    page_img = Image.open(io.BytesIO(img_data))\n                    \n                    # If this page has few extracted images but is large, save the page render\n                    page_images_count = len([m for m in metadata if m[\"page\"] == page_num + 1])\n                    if page_images_count < 2 and page_img.width > 800:  # Likely contains charts/diagrams\n                        save_path = os.path.join(output_dir, f\"page{page_num+1}_fullrender.png\")\n                        page_img.save(save_path, \"PNG\", quality=95)\n                        \n                        figures.append(save_path)\n                        metadata.append({\n                            \"page\": page_num + 1,\n                            \"method\": \"page_render\",\n                            \"size\": (page_img.width, page_img.height),\n                            \"original_format\": \"rendered\"\n                        })\n                        print(f\"  ‚úÖ Saved page render: {os.path.basename(save_path)}\")\n                    \n                    pix = None\n                    \n                except Exception as e:\n                    print(f\"  ‚ö†Ô∏è Error rendering page {page_num + 1}: {e}\")\n            \n            doc.close()\n            \n        except Exception as e:\n            print(f\"Error in image extraction: {e}\")\n            \n        print(f\"\\nüéØ TOTAL: {len(figures)} images extracted from PDF\")\n        return figures, metadata\n\n# Initialize and run enhanced extraction\npdf_path = \"/kaggle/input/edu-video/1-s2.0-S0957417424008327-main.pdf\"\n\nif os.path.exists(pdf_path):\n    extractor = EnhancedPDFImageExtractor(min_figure_size=3000)  # Lower threshold\n    figures, metadata = extractor.extract_all_images(pdf_path)\n    print(f\"\\nüéâ Extracted {len(figures)} figures total\")\n    \n    # Show breakdown\n    methods = {}\n    for m in metadata:\n        method = m[\"method\"]\n        methods[method] = methods.get(method, 0) + 1\n    \n    for method, count in methods.items():\n        print(f\"  - {method}: {count} images\")\n        \nelse:\n    print(f\"‚ùå PDF not found: {pdf_path}\")\n    figures, metadata = [], []\n\nprint(\"‚úÖ Cell 2 Complete - Enhanced image extraction finished\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:28:43.794366Z","iopub.execute_input":"2025-10-04T11:28:43.794674Z","iopub.status.idle":"2025-10-04T11:28:55.321877Z","shell.execute_reply.started":"2025-10-04T11:28:43.794654Z","shell.execute_reply":"2025-10-04T11:28:55.320716Z"}},"outputs":[{"name":"stdout","text":"Collecting PyMuPDF==1.23.26\n  Using cached PyMuPDF-1.23.26-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: PyMuPDFb==1.23.22 in /usr/local/lib/python3.11/dist-packages (from PyMuPDF==1.23.26) (1.23.22)\nUsing cached PyMuPDF-1.23.26-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\nInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.23.26\n‚úÖ PyMuPDF installed and working\nProcessing 13 pages for image extraction...\nPage 1: Found 3 potential images\n  ‚úÖ Saved: page1_img1.png (236x298)\n  ‚úÖ Saved: page1_img2.png (248x271)\n  ‚úÖ Saved: page1_img3.png (119x119)\nPage 2: Found 0 potential images\n  ‚úÖ Saved page render: page2_fullrender.png\nPage 3: Found 0 potential images\n  ‚úÖ Saved page render: page3_fullrender.png\nPage 4: Found 1 potential images\n  ‚úÖ Saved: page4_img6.png (1697x563)\n  ‚úÖ Saved page render: page4_fullrender.png\nPage 5: Found 1 potential images\n  ‚úÖ Saved: page5_img8.png (63x59)\n  ‚úÖ Saved page render: page5_fullrender.png\nPage 6: Found 0 potential images\n  ‚úÖ Saved page render: page6_fullrender.png\nPage 7: Found 0 potential images\n  ‚úÖ Saved page render: page7_fullrender.png\nPage 8: Found 1 potential images\n  ‚úÖ Saved: page8_img12.png (1647x617)\n  ‚úÖ Saved page render: page8_fullrender.png\nPage 9: Found 1 potential images\n  ‚úÖ Saved: page9_img14.png (2008x2251)\n  ‚úÖ Saved page render: page9_fullrender.png\nPage 10: Found 0 potential images\n  ‚úÖ Saved page render: page10_fullrender.png\nPage 11: Found 0 potential images\n  ‚úÖ Saved page render: page11_fullrender.png\nPage 12: Found 0 potential images\n  ‚úÖ Saved page render: page12_fullrender.png\nPage 13: Found 0 potential images\n  ‚úÖ Saved page render: page13_fullrender.png\n\nüéØ TOTAL: 19 images extracted from PDF\n\nüéâ Extracted 19 figures total\n  - direct_extract: 7 images\n  - page_render: 12 images\n‚úÖ Cell 2 Complete - Enhanced image extraction finished\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# CELL 3: Enhanced Image Explanation Generation (FIXED for Full Model Names)\n# ==============================================================================\n\nimport google.generativeai as genai\nimport re\n\n# Configure Gemini API\nGEMINI_API_KEY = \"AIzaSyD9AjDpmfsLWRhz5olrBmL3OLkEsqq6QcU\"\ngenai.configure(api_key=GEMINI_API_KEY)\n\ndef list_available_models():\n    \"\"\"List supported Gemini models for debugging\"\"\"\n    print(\"üîç Listing available Gemini models...\")\n    available_models = []\n    for model in genai.list_models():\n        if 'generateContent' in model.supported_generation_methods:\n            available_models.append(model.name)\n            print(f\"  - {model.name} (supports vision: {model.supported_generation_methods})\")\n    print(f\"\\nüìã Total supported models: {len(available_models)}\")\n    return available_models\n\ndef get_image_explanations(figures, metadata):\n    \"\"\"Generate educational explanations for extracted figures using a stable 2025 Gemini model\"\"\"\n    # List models first for debugging\n    available_models = list_available_models()\n    \n    # Choose a stable 2025 model (fallback chain with full model names)\n    model_name = None\n    if 'models/gemini-2.5-flash' in available_models:\n        model_name = 'gemini-2.5-flash'\n    elif 'models/gemini-2.0-flash' in available_models:\n        model_name = 'gemini-2.0-flash'\n    elif 'models/gemini-1.5-flash' in available_models:\n        model_name = 'gemini-1.5-flash'\n    else:\n        # Fallback to the first available vision-capable model\n        for m in available_models:\n            if 'gemini' in m and 'generateContent' in genai.list_models().get(m, {}).supported_generation_methods:\n                model_name = m.replace('models/', '')  # Strip 'models/' prefix\n                break\n        if not model_name:\n            raise ValueError(\"No suitable Gemini model found!\")\n    \n    print(f\"ü§ñ Using model: {model_name}\")\n    \n    diagrams = []\n    model = genai.GenerativeModel(model_name)\n    \n    print(f\"üîç Generating explanations for {len(figures)} figures...\")\n    \n    for i, fig_path in enumerate(figures):\n        try:\n            with Image.open(fig_path) as image:\n                # Prompt for educational explanation suitable for narration and display\n                prompt = \"\"\"\n                Analyze this image and provide a clear, educational explanation in 2-3 sentences.\n                First, briefly transcribe any key visible text (titles, labels, main data) to reference.\n                Then explain:\n                1. What type of visualization this is (chart, diagram, graph, etc.)\n                2. What key information or concept it presents\n                3. Any important data trends or relationships shown\n                \n                Make it suitable for an educational video narration and on-screen text. Be specific, informative, and engaging.\n                Output as a single cohesive paragraph.\n                \"\"\"\n                \n                response = model.generate_content([prompt, image])\n                explanation = response.text.strip()\n                \n                # Clean up for display and audio\n                explanation = re.sub(r'[*#]+', '', explanation)  # Remove markdown\n                explanation = re.sub(r'\\n+', ' ', explanation)  # Single line for audio\n                explanation_display = re.sub(r'\\s+', ' ', explanation).strip()  # Clean for text overlay\n                \n                diagrams.append({\n                    \"image_path\": fig_path, \n                    \"explanation_audio\": explanation,  # For TTS\n                    \"explanation_display\": explanation_display,  # For on-screen text below image\n                    \"page\": metadata[i][\"page\"] if i < len(metadata) else 1\n                })\n                \n                print(f\"‚úÖ Figure {i+1} (Page {diagrams[-1]['page']}): {explanation_display[:100]}...\")\n                \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error processing {fig_path}: {e}\")\n            # Add fallback explanation\n            page_num = metadata[i][\"page\"] if i < len(metadata) else i+1\n            fallback_explanation = f\"This figure from page {page_num} illustrates key visual information related to the research findings discussed in this section.\"\n            diagrams.append({\n                \"image_path\": fig_path,\n                \"explanation_audio\": fallback_explanation,\n                \"explanation_display\": fallback_explanation,\n                \"page\": page_num\n            })\n    \n    return diagrams\n\n# Generate explanations\nif figures:\n    diagrams = get_image_explanations(figures, metadata)\n    print(f\"\\n‚úÖ Generated {len(diagrams)} explanations\")\nelse:\n    diagrams = []\n    print(\"No figures to explain\")\n\nprint(\"‚úÖ Cell 3 Complete - Image explanations generated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:36:34.403023Z","iopub.execute_input":"2025-10-04T11:36:34.403465Z","iopub.status.idle":"2025-10-04T11:39:37.319996Z","shell.execute_reply.started":"2025-10-04T11:36:34.403437Z","shell.execute_reply":"2025-10-04T11:39:37.318775Z"}},"outputs":[{"name":"stdout","text":"üîç Listing available Gemini models...\n  - models/gemini-2.5-pro-preview-03-25 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-preview-05-20 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-lite-preview-06-17 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-pro-preview-05-06 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-pro-preview-06-05 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-pro (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-exp (supports vision: ['generateContent', 'countTokens', 'bidiGenerateContent'])\n  - models/gemini-2.0-flash (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-001 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-exp-image-generation (supports vision: ['generateContent', 'countTokens', 'bidiGenerateContent'])\n  - models/gemini-2.0-flash-lite-001 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-lite (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-preview-image-generation (supports vision: ['generateContent', 'countTokens', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-lite-preview-02-05 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-lite-preview (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-pro-exp (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-pro-exp-02-05 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-exp-1206 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-thinking-exp-01-21 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-thinking-exp (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.0-flash-thinking-exp-1219 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-preview-tts (supports vision: ['countTokens', 'generateContent'])\n  - models/gemini-2.5-pro-preview-tts (supports vision: ['countTokens', 'generateContent'])\n  - models/learnlm-2.0-flash-experimental (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3-1b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3-4b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3-12b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3-27b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3n-e4b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemma-3n-e2b-it (supports vision: ['generateContent', 'countTokens'])\n  - models/gemini-flash-latest (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-flash-lite-latest (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-pro-latest (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-lite (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-image-preview (supports vision: ['generateContent', 'countTokens'])\n  - models/gemini-2.5-flash-image (supports vision: ['generateContent', 'countTokens'])\n  - models/gemini-2.5-flash-preview-09-2025 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-2.5-flash-lite-preview-09-2025 (supports vision: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'])\n  - models/gemini-robotics-er-1.5-preview (supports vision: ['generateContent', 'countTokens'])\n\nüìã Total supported models: 40\nü§ñ Using model: gemini-2.5-flash\nüîç Generating explanations for 19 figures...\n‚úÖ Figure 1 (Page 1): This image displays the cover of \"Expert Systems with Applications: An International Journal,\" featu...\n‚úÖ Figure 2 (Page 1): This intricate woodcut illustration, bearing the text \"NON SOLUS\" and \"ELSEVIER,\" serves as the dist...\n‚úÖ Figure 3 (Page 1): There are no visible text labels or data elements within this image. This is a simple gray-scale gra...\n‚úÖ Figure 4 (Page 2): This image displays a page from a scholarly article, specifically a literature review from 'Expert S...\n‚úÖ Figure 5 (Page 3): This image presents two neural network architecture diagrams: \"Fig. 1. Structure diagram of the CBOW...\n‚úÖ Figure 6 (Page 4): This conceptual illustration, featuring \"Stock NEWS\" articles and candlestick charts alongside an ey...\n‚úÖ Figure 7 (Page 4): This flowchart, titled \"Process of the proposed stock market index prediction,\" illustrates a sophis...\n‚úÖ Figure 8 (Page 5): This image is a simple circular graphic, featuring a vibrant color gradient with no visible text or ...\n‚úÖ Figure 9 (Page 5): This composite image showcases machine learning model architectures, specifically the SA-TrellisNet ...\n‚úÖ Figure 10 (Page 6): This table, titled \"Table 4: Examples of sentiment analysis on SSEC news,\" showcases the output of a...\n‚úÖ Figure 11 (Page 7): This image presents a series of data tables, titled Tables 6 through 9, systematically comparing the...\n‚úÖ Figure 12 (Page 8): This image presents two line graphs, comparing the training performance of an LSTM model against an ...\n‚úÖ Figure 13 (Page 8): This image presents two comparative line graphs titled \"Loss In LSTM And LSTM-CNN\" and \"Accuracy In ...\n‚úÖ Figure 14 (Page 9): This visualization features a series of seven line graphs, comparing \"real price\" to \"predicted pric...\n‚úÖ Figure 15 (Page 9): This visualization presents a series of seven line graphs, each comparing the \"real price\" of a majo...\n‚úÖ Figure 16 (Page 10): This image presents a textual segment from a research paper, specifically sections like \"4. Experime...\n‚úÖ Figure 17 (Page 11): This image displays a page from an academic paper by W.-J. Liu et al., discussing a novel model call...\n‚úÖ Figure 18 (Page 12): This image displays a list of academic references, titled at the top \"W.-J. Liu et al.\" and \"Expert ...\n‚úÖ Figure 19 (Page 13): This image displays a segment of an academic bibliography or references list, rather than a traditio...\n\n‚úÖ Generated 19 explanations\n‚úÖ Cell 3 Complete - Image explanations generated\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: Skip PDF Text Extraction (Image-Centric Approach)\n# ==============================================================================\n\n# Skipping main PDF text extraction as we're focusing on per-image explanations for narration\nprint(\"‚ÑπÔ∏è Skipping PDF text extraction - narration will use image-specific explanations\")\npdf_content = \"\"  # Not used\nchunks = []  # Not used for audio\n\nprint(\"‚úÖ Cell 4 Complete - PDF text extraction skipped\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:44:17.963551Z","iopub.execute_input":"2025-10-04T11:44:17.965125Z","iopub.status.idle":"2025-10-04T11:44:17.971312Z","shell.execute_reply.started":"2025-10-04T11:44:17.965080Z","shell.execute_reply":"2025-10-04T11:44:17.970034Z"}},"outputs":[{"name":"stdout","text":"‚ÑπÔ∏è Skipping PDF text extraction - narration will use image-specific explanations\n‚úÖ Cell 4 Complete - PDF text extraction skipped\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: Skip Text Chunking (Image-Centric Approach)\n# ==============================================================================\n\n# Skipping text chunking as we're using per-image explanations for narration\nprint(\"‚ÑπÔ∏è Skipping text chunking - narration segments will use individual image explanations\")\n# Set chunks to explanations for compatibility (one per image)\nif 'diagrams' in locals():\n    chunks = [d['explanation_audio'] for d in diagrams]\nelse:\n    chunks = []\n\nprint(f\"‚úÖ Cell 5 Complete - Text chunking skipped; {len(chunks)} explanation segments ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:44:45.793528Z","iopub.execute_input":"2025-10-04T11:44:45.794793Z","iopub.status.idle":"2025-10-04T11:44:45.801005Z","shell.execute_reply.started":"2025-10-04T11:44:45.794752Z","shell.execute_reply":"2025-10-04T11:44:45.799928Z"}},"outputs":[{"name":"stdout","text":"‚ÑπÔ∏è Skipping text chunking - narration segments will use individual image explanations\n‚úÖ Cell 5 Complete - Text chunking skipped; 19 explanation segments ready\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: Professional Audio Generation from Image Explanations (FIXED)\n# ==============================================================================\n\nimport asyncio\nimport edge_tts\n\nasync def professional_text_to_speech(text, voice=\"en-US-JennyNeural\", rate=\"-5%\", pitch=\"+0Hz\"):\n    \"\"\"Convert text to professional narration using Edge TTS\"\"\"\n    # Clean text for speech\n    speech_text = text.strip()\n    \n    # Remove any remaining formatting artifacts\n    speech_text = re.sub(r'[*#_`]', '', speech_text)  # Markdown\n    speech_text = re.sub(r'http[s]?://\\S+', 'website link', speech_text)  # URLs\n    speech_text = re.sub(r'\\b[A-Z]{2,}\\b', lambda m: m.group().lower(), speech_text)  # ACRONYMS to lowercase\n    \n    # Improve pronunciation of common terms\n    replacements = {\n        'et al.': 'and colleagues',\n        'i.e.': 'that is',\n        'e.g.': 'for example',\n        'vs.': 'versus',\n        'etc.': 'and so on',\n        'COVID-19': 'COVID nineteen',\n        'AI': 'artificial intelligence',\n        'ML': 'machine learning',\n        'IoT': 'Internet of Things',\n        'API': 'A P I'\n    }\n    \n    for old, new in replacements.items():\n        speech_text = speech_text.replace(old, new)\n    \n    # Generate speech\n    tts = edge_tts.Communicate(speech_text, voice=voice, rate=rate, pitch=pitch)\n    audio_stream = b\"\"\n    async for chunk in tts.stream():\n        if chunk[\"type\"] == \"audio\":\n            audio_stream += chunk[\"data\"]\n    \n    return audio_stream\n\nasync def generate_explanation_audio(diagrams, output_dir=\"explanation_audio\"):\n    \"\"\"Generate audio files for each image's explanation\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    audio_paths = []\n    \n    # Professional voice options\n    voices = [\n        \"en-US-JennyNeural\",    # Clear, professional female voice\n        \"en-US-GuyNeural\",      # Clear, professional male voice  \n        \"en-US-AriaNeural\",     # Warm, engaging female voice\n        \"en-US-DavisNeural\"     # Confident male voice\n    ]\n    \n    selected_voice = voices[0]  # Use Jenny by default\n    print(f\"üéôÔ∏è Using voice: {selected_voice}\")\n    print(f\"üîä Generating audio from explanations for {len(diagrams)} images...\")\n    \n    for i, diagram in enumerate(diagrams):\n        explanation = diagram.get('explanation_audio', '').strip()\n        if not explanation:\n            continue\n            \n        out_path = os.path.join(output_dir, f\"image_{i+1:02d}_explanation.mp3\")\n        print(f\"üîÑ Processing image {i+1}/{len(diagrams)}...\")\n        \n        try:\n            audio_bytes = await professional_text_to_speech(\n                explanation, \n                voice=selected_voice, \n                rate=\"-5%\",  # Slightly slower for clarity\n                pitch=\"+0Hz\"\n            )\n            \n            with open(out_path, \"wb\") as f:\n                f.write(audio_bytes)\n            \n            audio_paths.append(out_path)\n            \n            # Calculate estimated duration (rough estimate)\n            word_count = len(explanation.split())\n            estimated_duration = word_count / 2.5  # ~2.5 words per second\n            print(f\"‚úÖ Audio saved: {os.path.basename(out_path)} (~{estimated_duration:.1f}s)\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error generating audio for image {i+1}: {e}\")\n            continue\n    \n    return audio_paths\n\n# Generate explanation audio files\nif diagrams:\n    audio_files = await generate_explanation_audio(diagrams)\n    print(f\"\\nüîä Generated {len(audio_files)} explanation audio files\")\n    \n    # Calculate total estimated duration\n    total_words = sum(len(d.get('explanation_audio', '').split()) for d in diagrams)\n    total_duration = total_words / 2.5 / 60  # Convert to minutes\n    print(f\"üìä Estimated total video duration: {total_duration:.1f} minutes\")\nelse:\n    audio_files = []\n    print(\"No diagrams available for audio generation\")\n\nprint(\"‚úÖ Cell 6 Complete - Explanation audio generation finished\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:45:22.272906Z","iopub.execute_input":"2025-10-04T11:45:22.273284Z","iopub.status.idle":"2025-10-04T11:45:41.056879Z","shell.execute_reply.started":"2025-10-04T11:45:22.273260Z","shell.execute_reply":"2025-10-04T11:45:41.055751Z"}},"outputs":[{"name":"stdout","text":"üéôÔ∏è Using voice: en-US-JennyNeural\nüîä Generating audio from explanations for 19 images...\nüîÑ Processing image 1/19...\n‚úÖ Audio saved: image_01_explanation.mp3 (~32.4s)\nüîÑ Processing image 2/19...\n‚úÖ Audio saved: image_02_explanation.mp3 (~28.8s)\nüîÑ Processing image 3/19...\n‚úÖ Audio saved: image_03_explanation.mp3 (~24.4s)\nüîÑ Processing image 4/19...\n‚úÖ Audio saved: image_04_explanation.mp3 (~28.4s)\nüîÑ Processing image 5/19...\n‚úÖ Audio saved: image_05_explanation.mp3 (~38.4s)\nüîÑ Processing image 6/19...\n‚úÖ Audio saved: image_06_explanation.mp3 (~27.2s)\nüîÑ Processing image 7/19...\n‚úÖ Audio saved: image_07_explanation.mp3 (~31.6s)\nüîÑ Processing image 8/19...\n‚úÖ Audio saved: image_08_explanation.mp3 (~27.6s)\nüîÑ Processing image 9/19...\n‚úÖ Audio saved: image_09_explanation.mp3 (~36.0s)\nüîÑ Processing image 10/19...\n‚úÖ Audio saved: image_10_explanation.mp3 (~32.8s)\nüîÑ Processing image 11/19...\n‚úÖ Audio saved: image_11_explanation.mp3 (~32.8s)\nüîÑ Processing image 12/19...\n‚úÖ Audio saved: image_12_explanation.mp3 (~30.4s)\nüîÑ Processing image 13/19...\n‚úÖ Audio saved: image_13_explanation.mp3 (~39.2s)\nüîÑ Processing image 14/19...\n‚úÖ Audio saved: image_14_explanation.mp3 (~26.8s)\nüîÑ Processing image 15/19...\n‚úÖ Audio saved: image_15_explanation.mp3 (~36.0s)\nüîÑ Processing image 16/19...\n‚úÖ Audio saved: image_16_explanation.mp3 (~38.4s)\nüîÑ Processing image 17/19...\n‚úÖ Audio saved: image_17_explanation.mp3 (~31.2s)\nüîÑ Processing image 18/19...\n‚úÖ Audio saved: image_18_explanation.mp3 (~34.0s)\nüîÑ Processing image 19/19...\n‚úÖ Audio saved: image_19_explanation.mp3 (~29.2s)\n\nüîä Generated 19 explanation audio files\nüìä Estimated total video duration: 10.1 minutes\n‚úÖ Cell 6 Complete - Explanation audio generation finished\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: Professional Video Generation Class (FIXED for Per-Image Explanation Sync)\n# ==============================================================================\n\nfrom moviepy.editor import *\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nimport os\n\nclass ProfessionalVideoGenerator:\n    def __init__(self, width=1920, height=1080, fps=30):  # Full HD, smooth playback\n        self.width = width\n        self.height = height\n        self.fps = fps\n        self.bg_color = (15, 23, 42)  # Professional dark blue\n        self.text_color = (248, 250, 252)  # Off-white\n        self.accent_color = (59, 130, 246)  # Blue accent\n        \n    def load_professional_font(self, fontsize, bold=False):\n        \"\"\"Load professional font with multiple fallbacks\"\"\"\n        font_paths = [\n            \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\" if bold else \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n            \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\" if bold else \"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\",\n            \"/System/Library/Fonts/Helvetica.ttc\",\n            \"/Windows/Fonts/arial.ttf\"\n        ]\n        \n        for font_path in font_paths:\n            try:\n                return ImageFont.truetype(font_path, fontsize)\n            except:\n                continue\n        \n        return ImageFont.load_default()\n    \n    def wrap_text_professional(self, text, font, max_width):\n        \"\"\"Professional text wrapping with better line breaks\"\"\"\n        words = text.split()\n        lines = []\n        current_line = []\n        \n        for word in words:\n            test_line = ' '.join(current_line + [word])\n            try:\n                bbox = font.getbbox(test_line)\n                line_width = bbox[2] - bbox[0]\n            except:\n                line_width = len(test_line) * (font.size * 0.6)\n            \n            if line_width <= max_width:\n                current_line.append(word)\n            else:\n                if current_line:\n                    lines.append(' '.join(current_line))\n                current_line = [word]\n        \n        if current_line:\n            lines.append(' '.join(current_line))\n        \n        return lines\n    \n    def create_title_slide(self, title, duration=4):\n        \"\"\"Create professional title slide\"\"\"\n        def make_title_frame(t):\n            img = Image.new('RGB', (self.width, self.height), self.bg_color)\n            draw = ImageDraw.Draw(img)\n            \n            # Add gradient-like effect\n            for i in range(self.height//3):\n                alpha = int(255 * (i / (self.height//3)) * 0.1)\n                color = (self.bg_color[0] + alpha//4, self.bg_color[1] + alpha//4, self.bg_color[2] + alpha//2)\n                draw.rectangle([0, i*3, self.width, i*3+3], fill=color)\n            \n            # Title\n            title_font = self.load_professional_font(72, bold=True)\n            title_lines = self.wrap_text_professional(title, title_font, self.width - 200)\n            \n            total_title_height = len(title_lines) * 80\n            start_y = (self.height - total_title_height) // 2\n            \n            for i, line in enumerate(title_lines):\n                try:\n                    bbox = draw.textbbox((0, 0), line, font=title_font)\n                    text_width = bbox[2] - bbox[0]\n                except:\n                    text_width = len(line) * 40\n                \n                x = (self.width - text_width) // 2\n                y = start_y + i * 80\n                \n                # Text shadow\n                draw.text((x+3, y+3), line, fill=(0, 0, 0, 128), font=title_font)\n                # Main text\n                draw.text((x, y), line, fill=self.text_color, font=title_font)\n            \n            # Accent line\n            line_y = start_y + total_title_height + 30\n            draw.rectangle([self.width//4, line_y, 3*self.width//4, line_y+5], fill=self.accent_color)\n            \n            return np.array(img)\n        \n        return VideoClip(make_title_frame, duration=duration)\n    \n    def create_image_slide(self, image_path, explanation_display, duration, image_number):\n        \"\"\"Create slide for one image with explanation displayed below\"\"\"\n        def make_image_frame(t):\n            img = Image.new('RGB', (self.width, self.height), self.bg_color)\n            draw = ImageDraw.Draw(img)\n            \n            # Image number in top-right corner\n            num_font = self.load_professional_font(24)\n            num_text = f\"Figure {image_number}\"\n            draw.text((self.width - 200, 30), num_text, fill=self.accent_color, font=num_font)\n            \n            # Load and process image (upper 60% of screen)\n            try:\n                with Image.open(image_path) as content_img:\n                    if content_img.mode != 'RGB':\n                        content_img = content_img.convert('RGB')\n                    \n                    # Calculate scaling to fit in upper 60% of screen\n                    available_width = self.width - 100\n                    available_height = int(self.height * 0.6) - 50\n                    \n                    # Scale image proportionally\n                    img_ratio = content_img.width / content_img.height\n                    area_ratio = available_width / available_height\n                    \n                    if img_ratio > area_ratio:\n                        # Image is wider - fit to width\n                        new_width = available_width\n                        new_height = int(available_width / img_ratio)\n                    else:\n                        # Image is taller - fit to height\n                        new_height = available_height\n                        new_width = int(available_height * img_ratio)\n                    \n                    content_img_resized = content_img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n                    \n                    # Center image in upper area\n                    img_x = (self.width - new_width) // 2\n                    img_y = 80 + (available_height - new_height) // 2\n                    \n                    # Add subtle border and shadow\n                    shadow_offset = 8\n                    shadow_color = (0, 0, 0, 60)\n                    \n                    # Shadow\n                    shadow_img = Image.new('RGBA', (new_width, new_height), shadow_color)\n                    img.paste(shadow_img, (img_x + shadow_offset, img_y + shadow_offset), shadow_img)\n                    \n                    # Border\n                    border_color = (71, 85, 105)\n                    draw.rectangle([img_x-3, img_y-3, img_x+new_width+3, img_y+new_height+3], \n                                 outline=border_color, width=3)\n                    \n                    # Paste main image\n                    img.paste(content_img_resized, (img_x, img_y))\n                    \n            except Exception as e:\n                print(f\"Error loading image {image_path}: {e}\")\n                # Fallback: show placeholder\n                placeholder_font = self.load_professional_font(36)\n                placeholder_text = f\"Figure {image_number}: Image not available\"\n                placeholder_color = (100, 100, 100)\n                \n                # Draw placeholder rectangle\n                placeholder_width = 600\n                placeholder_height = 400\n                placeholder_x = (self.width - placeholder_width) // 2\n                placeholder_y = 150\n                \n                draw.rectangle([placeholder_x, placeholder_y, \n                              placeholder_x + placeholder_width, \n                              placeholder_y + placeholder_height], \n                             fill=placeholder_color, outline=border_color, width=2)\n                \n                # Center text in placeholder\n                try:\n                    bbox = draw.textbbox((0, 0), placeholder_text, font=placeholder_font)\n                    text_width = bbox[2] - bbox[0]\n                    text_height = bbox[3] - bbox[1]\n                except:\n                    text_width = len(placeholder_text) * 20\n                    text_height = 36\n                \n                text_x = placeholder_x + (placeholder_width - text_width) // 2\n                text_y = placeholder_y + (placeholder_height - text_height) // 2\n                draw.text((text_x, text_y), placeholder_text, fill=self.text_color, font=placeholder_font)\n            \n            # Explanation area (lower 35% of screen) - displayed below image\n            desc_start_y = int(self.height * 0.68)\n            desc_area_height = self.height - desc_start_y - 50\n            \n            # Background panel for explanation\n            panel_margin = 40\n            panel_color = (30, 41, 59)\n            draw.rectangle([panel_margin, desc_start_y, self.width - panel_margin, self.height - 30], \n                         fill=panel_color, outline=(71, 85, 105), width=2)\n            \n            # Explanation text\n            if explanation_display and explanation_display.strip():\n                expl_font = self.load_professional_font(28)  # Font for explanation text\n                expl_lines = self.wrap_text_professional(explanation_display, expl_font, self.width - 120)\n                \n                line_height = 38\n                total_text_height = len(expl_lines) * line_height\n                \n                # Center text vertically in explanation area\n                text_start_y = desc_start_y + (desc_area_height - total_text_height) // 2 + 15\n                \n                for i, line in enumerate(expl_lines):\n                    y_pos = text_start_y + i * line_height\n                    # Add subtle text shadow for readability\n                    draw.text((62, y_pos + 2), line, fill=(0, 0, 0, 100), font=expl_font)\n                    draw.text((60, y_pos), line, fill=self.text_color, font=expl_font)\n            \n            return np.array(img)\n        \n        return VideoClip(make_image_frame, duration=duration)\n    \n    def create_end_slide(self, duration=3):\n        \"\"\"Create professional end slide\"\"\"\n        def make_end_frame(t):\n            img = Image.new('RGB', (self.width, self.height), self.bg_color)\n            draw = ImageDraw.Draw(img)\n            \n            # Gradient effect\n            for i in range(self.height//2):\n                alpha = int(255 * (1 - i / (self.height//2)) * 0.15)\n                color = (self.bg_color[0] + alpha//3, self.bg_color[1] + alpha//3, self.bg_color[2] + alpha//2)\n                draw.rectangle([0, i*2, self.width, i*2+2], fill=color)\n            \n            # Thank you message\n            end_font = self.load_professional_font(64, bold=True)\n            end_text = \"Thank You for Watching\"\n            \n            try:\n                bbox = draw.textbbox((0, 0), end_text, font=end_font)\n                text_width = bbox[2] - bbox[0]\n            except:\n                text_width = len(end_text) * 35\n            \n            x = (self.width - text_width) // 2\n            y = (self.height - 70) // 2\n            \n            # Text with shadow\n            draw.text((x+4, y+4), end_text, fill=(0, 0, 0, 128), font=end_font)\n            draw.text((x, y), end_text, fill=self.text_color, font=end_font)\n            \n            # Decorative elements\n            accent_y = y + 100\n            draw.rectangle([self.width//4, accent_y, 3*self.width//4, accent_y+6], fill=self.accent_color)\n            \n            return np.array(img)\n        \n        return VideoClip(make_end_frame, duration=duration)\n    \n    def generate_professional_video(self, diagrams, audio_files, \n                                   output_path=\"professional_image_explanation_video.mp4\", \n                                   title=\"PDF Images with Explanations\"):\n        \"\"\"Generate video with one segment per image: image + explanation below + synced audio\"\"\"\n        \n        print(f\"üé¨ Generating professional video...\")\n        print(f\"  üñºÔ∏è Images: {len(diagrams)}\")\n        print(f\"  üîä Audio files: {len(audio_files)}\")\n        \n        all_clips = []\n        \n        # 1. Title slide\n        print(\"üéØ Creating title slide...\")\n        title_clip = self.create_title_slide(title, duration=4)\n        all_clips.append(title_clip)\n        \n        # 2. One content segment per image\n        for i, (diagram, audio_file) in enumerate(zip(diagrams, audio_files)):\n            print(f\"üîÑ Processing image {i+1}/{len(diagrams)}...\")\n            \n            try:\n                # Load audio to get exact duration\n                audio_clip = AudioFileClip(audio_file)\n                segment_duration = audio_clip.duration\n                \n                # Create video clip for this image with explanation\n                image_clip = self.create_image_slide(\n                    diagram[\"image_path\"],\n                    diagram[\"explanation_display\"],\n                    segment_duration,\n                    i + 1\n                )\n                \n                # Sync audio to video\n                synced_clip = image_clip.set_audio(audio_clip)\n                all_clips.append(synced_clip)\n                print(f\"  ‚úÖ Image {i+1} segment created (duration: {segment_duration:.1f}s)\")\n                \n            except Exception as e:\n                print(f\"  ‚ùå Error creating image {i+1}: {e}\")\n                continue\n        \n        # 3. End slide\n        print(\"üéØ Creating end slide...\")\n        end_clip = self.create_end_slide(duration=3)\n        all_clips.append(end_clip)\n        \n        if not all_clips:\n            print(\"‚ùå No clips were created!\")\n            return None\n        \n        print(\"üîÑ Assembling final video...\")\n        try:\n            # Combine all clips\n            final_video = concatenate_videoclips(all_clips, method=\"compose\")\n            \n            # Calculate total duration\n            total_duration = sum(clip.duration for clip in all_clips)\n            print(f\"üìä Total video duration: {total_duration/60:.1f} minutes\")\n            \n            # Export with high quality settings\n            print(\"üíæ Exporting video (this may take a while)...\")\n            \n            export_params = {\n                'fps': self.fps,\n                'codec': 'libx264',\n                'preset': 'medium',  # Good quality/speed balance\n                'audio_codec': 'aac',\n                'bitrate': '4000k',  # High quality\n                'audio_bitrate': '192k',\n                'temp_audiofile': 'temp-audio.m4a',\n                'remove_temp': True,\n                'verbose': False,\n                'logger': None,\n                'threads': 4\n            }\n            \n            try:\n                final_video.write_videofile(output_path, **export_params)\n                print(f\"‚úÖ High-quality video exported successfully!\")\n                \n            except Exception as export_error:\n                print(f\"‚ö†Ô∏è High-quality export failed: {export_error}\")\n                print(\"üîÑ Trying with fallback settings...\")\n                \n                # Fallback export settings\n                fallback_params = {\n                    'fps': 24,\n                    'codec': 'libx264',\n                    'preset': 'fast',\n                    'bitrate': '2000k',\n                    'verbose': False,\n                    'logger': None\n                }\n                \n                final_video.write_videofile(output_path, **fallback_params)\n                print(f\"‚úÖ Video exported with fallback settings!\")\n            \n            # File info\n            if os.path.exists(output_path):\n                file_size = os.path.getsize(output_path) / (1024 * 1024)\n                print(f\"üìÅ Final video: {output_path}\")\n                print(f\"üìè File size: {file_size:.1f} MB\")\n                print(f\"üé• Resolution: {self.width}x{self.height}\")\n                print(f\"‚è±Ô∏è Duration: {total_duration/60:.1f} minutes\")\n            \n        except Exception as e:\n            print(f\"‚ùå Video assembly failed: {e}\")\n            return None\n        \n        finally:\n            # Cleanup\n            print(\"üßπ Cleaning up...\")\n            for clip in all_clips:\n                try:\n                    clip.close()\n                except:\n                    pass\n            \n            try:\n                final_video.close()\n            except:\n                pass\n        \n        return output_path\n\n# Initialize the generator (but don't generate yet - wait for Cell 8)\nvideo_generator = ProfessionalVideoGenerator()\n\nprint(\"‚úÖ Cell 7 Complete - Professional video generator class created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:46:35.341012Z","iopub.execute_input":"2025-10-04T11:46:35.341468Z","iopub.status.idle":"2025-10-04T11:46:35.393208Z","shell.execute_reply.started":"2025-10-04T11:46:35.341436Z","shell.execute_reply":"2025-10-04T11:46:35.392110Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Cell 7 Complete - Professional video generator class created\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ==============================================================================\n# CELL 8: Generate Final Professional Image Explanation Video\n# ==============================================================================\n\ndef create_professional_explanation_video():\n    \"\"\"Create the complete professional video with image explanations and synced audio\"\"\"\n    \n    print(\"üé¨ STARTING IMAGE EXPLANATION VIDEO GENERATION\")\n    print(\"=\" * 60)\n    \n    # Verify we have all required components\n    missing_components = []\n    \n    if not diagrams:\n        missing_components.append(\"explanations/diagrams\")\n    if not audio_files:\n        missing_components.append(\"audio files\")\n    if not figures:\n        missing_components.append(\"figures\")\n    \n    if missing_components:\n        print(f\"‚ùå Missing components: {', '.join(missing_components)}\")\n        return None\n    \n    # Component summary\n    print(f\"üìä COMPONENT SUMMARY:\")\n    print(f\"  üñºÔ∏è Images: {len(figures)}\")\n    print(f\"  üìù Explanations: {len(diagrams)}\")\n    print(f\"  üîä Audio files: {len(audio_files)}\")\n    \n    # Verify audio files exist\n    valid_audio_files = []\n    for audio_file in audio_files:\n        if os.path.exists(audio_file):\n            valid_audio_files.append(audio_file)\n            # Get file size for verification\n            size_mb = os.path.getsize(audio_file) / (1024 * 1024)\n            print(f\"  ‚úÖ {os.path.basename(audio_file)} ({size_mb:.1f} MB)\")\n        else:\n            print(f\"  ‚ö†Ô∏è Missing: {audio_file}\")\n    \n    if not valid_audio_files:\n        print(\"‚ùå No valid audio files found!\")\n        return None\n    \n    # Verify diagram/image files exist\n    valid_diagrams = []\n    for diagram in diagrams:\n        if os.path.exists(diagram[\"image_path\"]):\n            valid_diagrams.append(diagram)\n        else:\n            print(f\"  ‚ö†Ô∏è Missing image: {diagram['image_path']}\")\n    \n    print(f\"  üìÅ Valid diagrams: {len(valid_diagrams)}\")\n    \n    # Show estimated video duration\n    total_segments = min(len(valid_diagrams), len(valid_audio_files))\n    estimated_content_duration = sum(len(d.get('explanation_audio', '').split()) for d in valid_diagrams[:total_segments]) / 2.5  # words per second\n    estimated_total_duration = estimated_content_duration + 7  # + title + end slides\n    \n    print(f\"  ‚è±Ô∏è Estimated duration: {estimated_total_duration/60:.1f} minutes\")\n    print()\n    \n    # Initialize professional video generator\n    print(\"üéØ Initializing Professional Video Generator...\")\n    video_generator = ProfessionalVideoGenerator(\n        width=1920, \n        height=1080, \n        fps=30\n    )\n    \n    # Generate the professional video\n    print(\"üöÄ Starting video generation process...\")\n    output_path = video_generator.generate_professional_video(\n        diagrams=valid_diagrams[:len(valid_audio_files)],  # Match to available audio\n        audio_files=valid_audio_files,\n        output_path=\"professional_image_explanation_video.mp4\",\n        title=\"PDF Images with AI Explanations\"\n    )\n    \n    return output_path\n\ndef verify_final_video(video_path):\n    \"\"\"Verify the final video was created successfully\"\"\"\n    if not video_path:\n        print(\"‚ùå Video generation failed - no output path returned\")\n        return False\n    \n    if not os.path.exists(video_path):\n        print(f\"‚ùå Video file not found: {video_path}\")\n        return False\n    \n    # Get file info\n    file_size_mb = os.path.getsize(video_path) / (1024 * 1024)\n    \n    print(\"üéâ VIDEO GENERATION SUCCESSFUL!\")\n    print(\"=\" * 50)\n    print(f\"üìÅ File: {video_path}\")\n    print(f\"üíæ Size: {file_size_mb:.1f} MB\")\n    print(f\"üé• Resolution: 1920x1080 (Full HD)\")\n    print(f\"üé¨ Format: MP4 (H.264)\")\n    \n    # Verify it's not corrupted (basic check)\n    if file_size_mb > 5:  # Should be at least 5MB for a real video\n        print(\"‚úÖ File appears to be valid\")\n        print(\"üéØ Ready for download and viewing!\")\n        return True\n    else:\n        print(\"‚ö†Ô∏è File seems too small - may be corrupted\")\n        return False\n\n# Execute the complete video generation pipeline\nprint(\"üé¨ PROFESSIONAL IMAGE EXPLANATION VIDEO GENERATOR\")\nprint(\"=\" * 60)\nprint(\"Starting complete video generation pipeline...\")\nprint()\n\ntry:\n    # Generate the video\n    final_video_path = create_professional_explanation_video()\n    \n    if final_video_path:\n        # Verify the result\n        success = verify_final_video(final_video_path)\n        \n        if success:\n            print(\"\\nüéä PIPELINE COMPLETED SUCCESSFULLY!\")\n            print(\"Your professional image explanation video is ready!\")\n        else:\n            print(\"\\n‚ö†Ô∏è Pipeline completed but video may have issues\")\n    else:\n        print(\"\\n‚ùå PIPELINE FAILED\")\n        print(\"Please check the error messages above\")\n\nexcept Exception as e:\n    print(f\"\\nüí• UNEXPECTED ERROR: {e}\")\n    print(\"Pipeline failed with exception\")\n\nprint(\"\\n‚úÖ Cell 8 Complete - Video generation pipeline executed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:47:26.045730Z","iopub.execute_input":"2025-10-04T11:47:26.046162Z","iopub.status.idle":"2025-10-04T13:07:51.013275Z","shell.execute_reply.started":"2025-10-04T11:47:26.046130Z","shell.execute_reply":"2025-10-04T13:07:51.012117Z"}},"outputs":[{"name":"stdout","text":"üé¨ PROFESSIONAL IMAGE EXPLANATION VIDEO GENERATOR\n============================================================\nStarting complete video generation pipeline...\n\nüé¨ STARTING IMAGE EXPLANATION VIDEO GENERATION\n============================================================\nüìä COMPONENT SUMMARY:\n  üñºÔ∏è Images: 19\n  üìù Explanations: 19\n  üîä Audio files: 19\n  ‚úÖ image_01_explanation.mp3 (0.2 MB)\n  ‚úÖ image_02_explanation.mp3 (0.2 MB)\n  ‚úÖ image_03_explanation.mp3 (0.1 MB)\n  ‚úÖ image_04_explanation.mp3 (0.2 MB)\n  ‚úÖ image_05_explanation.mp3 (0.2 MB)\n  ‚úÖ image_06_explanation.mp3 (0.2 MB)\n  ‚úÖ image_07_explanation.mp3 (0.2 MB)\n  ‚úÖ image_08_explanation.mp3 (0.2 MB)\n  ‚úÖ image_09_explanation.mp3 (0.3 MB)\n  ‚úÖ image_10_explanation.mp3 (0.2 MB)\n  ‚úÖ image_11_explanation.mp3 (0.2 MB)\n  ‚úÖ image_12_explanation.mp3 (0.2 MB)\n  ‚úÖ image_13_explanation.mp3 (0.3 MB)\n  ‚úÖ image_14_explanation.mp3 (0.2 MB)\n  ‚úÖ image_15_explanation.mp3 (0.3 MB)\n  ‚úÖ image_16_explanation.mp3 (0.3 MB)\n  ‚úÖ image_17_explanation.mp3 (0.2 MB)\n  ‚úÖ image_18_explanation.mp3 (0.3 MB)\n  ‚úÖ image_19_explanation.mp3 (0.2 MB)\n  üìÅ Valid diagrams: 19\n  ‚è±Ô∏è Estimated duration: 10.2 minutes\n\nüéØ Initializing Professional Video Generator...\nüöÄ Starting video generation process...\nüé¨ Generating professional video...\n  üñºÔ∏è Images: 19\n  üîä Audio files: 19\nüéØ Creating title slide...\nüîÑ Processing image 1/19...\n  ‚úÖ Image 1 segment created (duration: 36.4s)\nüîÑ Processing image 2/19...\n  ‚úÖ Image 2 segment created (duration: 30.4s)\nüîÑ Processing image 3/19...\n  ‚úÖ Image 3 segment created (duration: 24.5s)\nüîÑ Processing image 4/19...\n  ‚úÖ Image 4 segment created (duration: 36.2s)\nüîÑ Processing image 5/19...\n  ‚úÖ Image 5 segment created (duration: 39.7s)\nüîÑ Processing image 6/19...\n  ‚úÖ Image 6 segment created (duration: 32.8s)\nüîÑ Processing image 7/19...\n  ‚úÖ Image 7 segment created (duration: 36.9s)\nüîÑ Processing image 8/19...\n  ‚úÖ Image 8 segment created (duration: 28.2s)\nüîÑ Processing image 9/19...\n  ‚úÖ Image 9 segment created (duration: 46.2s)\nüîÑ Processing image 10/19...\n  ‚úÖ Image 10 segment created (duration: 37.6s)\nüîÑ Processing image 11/19...\n  ‚úÖ Image 11 segment created (duration: 41.7s)\nüîÑ Processing image 12/19...\n  ‚úÖ Image 12 segment created (duration: 38.6s)\nüîÑ Processing image 13/19...\n  ‚úÖ Image 13 segment created (duration: 50.7s)\nüîÑ Processing image 14/19...\n  ‚úÖ Image 14 segment created (duration: 32.7s)\nüîÑ Processing image 15/19...\n  ‚úÖ Image 15 segment created (duration: 44.6s)\nüîÑ Processing image 16/19...\n  ‚úÖ Image 16 segment created (duration: 47.8s)\nüîÑ Processing image 17/19...\n  ‚úÖ Image 17 segment created (duration: 38.2s)\nüîÑ Processing image 18/19...\n  ‚úÖ Image 18 segment created (duration: 43.7s)\nüîÑ Processing image 19/19...\n  ‚úÖ Image 19 segment created (duration: 36.6s)\nüéØ Creating end slide...\nüîÑ Assembling final video...\nüìä Total video duration: 12.2 minutes\nüíæ Exporting video (this may take a while)...\n‚úÖ High-quality video exported successfully!\nüìÅ Final video: professional_image_explanation_video.mp4\nüìè File size: 46.2 MB\nüé• Resolution: 1920x1080\n‚è±Ô∏è Duration: 12.2 minutes\nüßπ Cleaning up...\nüéâ VIDEO GENERATION SUCCESSFUL!\n==================================================\nüìÅ File: professional_image_explanation_video.mp4\nüíæ Size: 46.2 MB\nüé• Resolution: 1920x1080 (Full HD)\nüé¨ Format: MP4 (H.264)\n‚úÖ File appears to be valid\nüéØ Ready for download and viewing!\n\nüéä PIPELINE COMPLETED SUCCESSFULLY!\nYour professional image explanation video is ready!\n\n‚úÖ Cell 8 Complete - Video generation pipeline executed\n","output_type":"stream"}],"execution_count":20}]}